{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: tracklets and bounding boxes are in one place after extracting the .gz file written during the tacking session. Now, we run keypoint inference on the input video using the tracklets we have extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sparrow_datums import Boxes\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "import io\n",
    "import torch\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from speed_trapv3.keypoints.model import SegmentationModel\n",
    "from speed_trapv3.keypoints.config import Config as KeyConfig\n",
    "from sparrow_datums import AugmentedBoxTracking, BoxTracking, FrameBoxes, PType\n",
    "from speed_trapv3.keypoints.dataset import crop_and_resize,process_keypoints, keypoints_post_inference_processing\n",
    "from speed_trapv3.tracking.tracking import get_video_properties, transform_image, write_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/code/data/datasets/common_hall/source_video/25_resampled_vid.mp4\"\n",
    "save_path = '/code/data/datasets/common_hall/tracking_inference_frame_lib/hard_coded_video2.mp4'\n",
    "gz_path = '/code/data/datasets/tracking/predictions/hard_coded/hard_coded_vehicle.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf /code/data/datasets/common_hall/tracking_inference_frame_lib/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps, n_frames = get_video_properties(video_path)\n",
    "# reader = imageio.get_reader(video_path)\n",
    "# slug = \"hard_coded\"\n",
    "# for i in tqdm(range(n_frames)):\n",
    "#     img = reader.get_data(i)\n",
    "#     img = cv2.rectangle(\n",
    "#         img, (450, 200), (1280, 720), thickness=5, color=(0, 255, 0))\n",
    "#     filename = str(i) + \".jpg\"\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     cv2.imwrite(os.path.join(save_path, filename), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint_model = SegmentationModel().eval().cuda()\n",
    "keypoint_model.load(KeyConfig.trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599it [04:20,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = imageio.get_reader(video_path)\n",
    "fps = reader.get_meta_data()[\"fps\"]\n",
    "class_label: bool = False\n",
    "score_label: bool = False\n",
    "object_label: bool = True\n",
    "vehicle_chunk = AugmentedBoxTracking.from_box_tracking(\n",
    "    BoxTracking.from_file(gz_path)\n",
    ")\n",
    "image_transform = T.Compose([T.ToTensor()])\n",
    "# if vehicle_chunk.fps != fps:\n",
    "#     vehicle_chunk = vehicle_chunk.resample(fps)\n",
    "with imageio.get_writer(\n",
    "    save_path, mode=\"I\", fps=fps, macro_block_size=None\n",
    ") as writer:\n",
    "    aggregated_predictions = [] #Len is equal to to the number of frames.\n",
    "    frame_idx = 0\n",
    "    for img, vehicle_boxes in tqdm(zip(reader, vehicle_chunk)):\n",
    "        frame_log = {}\n",
    "        frame_log['frame_idx'] = frame_idx\n",
    "        frame_log['annotations'] = []\n",
    "        # boxes = list(vehicle_boxes)\n",
    "        boxes = vehicle_boxes  # vehicle_boxes is a len = 16 list where unavailable objects are nan.\n",
    "        # result = plot_boxes(img, boxes, object_label=True)\n",
    "        height, width = img.shape[:2]\n",
    "        fig = plt.figure(frameon=False, figsize=(width / 100, height / 100), dpi=100)\n",
    "        fig.add_axes((0, 0, 1, 1))\n",
    "        plt.imshow(img)\n",
    "        # for boxes in tqdm(boxes_in):\n",
    "        #     print(\"Madona says boxes\", boxes)\n",
    "        for i, box in enumerate(boxes.to_absolute()):\n",
    "            object_log = {}\n",
    "            # for i, box in enumerate(boxes.to_absolute()):\n",
    "            if not np.isfinite(box.x):\n",
    "                continue\n",
    "            x1 = np.clip(box.x1, 2, width - 2)\n",
    "            x2 = np.clip(box.x2, 2, width - 2)\n",
    "            y1 = np.clip(box.y1, 2, height - 2)\n",
    "            y2 = np.clip(box.y2, 2, height - 2)\n",
    "            color: Optional[str] = None\n",
    "            text_strings: list[str] = []\n",
    "            if class_label:\n",
    "                text_strings.append(f\"class: {int(box.label)}\")\n",
    "                color = f\"C{int(box.label)}\"\n",
    "            if score_label:\n",
    "                text_strings.append(f\"score: {box.score:.2f}\")\n",
    "            if object_label:\n",
    "                text_strings.append(f\"object_id: {i}\")\n",
    "                if color is None:\n",
    "                    color = f\"C{i}\"\n",
    "            if color is None:\n",
    "                color = \"C0\"\n",
    "            plt.text(\n",
    "                x1 + 3,\n",
    "                y1 - 8,\n",
    "                \", \".join(text_strings),\n",
    "                backgroundcolor=(1, 1, 1, 0.5),\n",
    "                c=\"black\",\n",
    "                size=8,\n",
    "            )\n",
    "            plt.plot([x1, x2, x2, x1, x1], [y1, y1, y2, y2, y1], lw=2, c=color)\n",
    "            roi_w = x2 - x1\n",
    "            roi_h = y2 - y1\n",
    "            roi_resized = crop_and_resize(box.to_relative().array[:4], img, KeyConfig.image_crop_size[0],KeyConfig.image_crop_size[1])\n",
    "            roi_resized_w, roi_resized_h = roi_resized.size\n",
    "            x = image_transform(roi_resized)\n",
    "            x = torch.unsqueeze(x, 0).cuda()\n",
    "            keypoints = keypoint_model(x)['keypoints'][0].detach().cpu().numpy()\n",
    "            keypoints = keypoints_post_inference_processing(\n",
    "                    keypoints, roi_resized_w, roi_resized_h, roi_w, roi_h, x1, y1\n",
    "                )\n",
    "            object_log['keypoints'] = [list(keypoints[0]), list(keypoints[1])]\n",
    "            object_log['object_id'] = i\n",
    "            object_log['bounding_box'] = list(box.array[:4])\n",
    "            frame_log[\"annotations\"].append(object_log)\n",
    "            # plt.Circle((keypoints[0][0], keypoints[0][1]), 20, color='r')\n",
    "            # plt.Circle((keypoints[1][0], keypoints[1][1]), 20, color='b')\n",
    "            plt.plot(keypoints[0][0], keypoints[0][1], marker='o', color=\"red\")\n",
    "            plt.plot(keypoints[1][0], keypoints[1][1], marker='o', color=\"blue\")\n",
    "        aggregated_predictions.append(frame_log)\n",
    "        frame_idx += 1\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format=\"png\")\n",
    "        plt.close()\n",
    "        frame = imageio.v2.imread(buffer.getbuffer(), format=\"png\")\n",
    "        writer.append_data(frame)\n",
    "    out_file = open(\"/code/data/datasets/common_hall/tracking_output_videos/myfile.json\", \"w\")\n",
    "    json.dump(aggregated_predictions, out_file)\n",
    "    out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
