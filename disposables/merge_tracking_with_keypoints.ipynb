{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: tracklets and bounding boxes are in one place after extracting the .gz file written during the tacking session. Now, we run keypoint inference on the input video using the tracklets we have extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sparrow_datums import Boxes\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from speed_trapv3.keypoints.model import SegmentationModel\n",
    "from speed_trapv3.keypoints.config import Config as KeyConfig\n",
    "from speed_trapv3.keypoints.dataset import crop_and_resize,process_keypoints, keypoints_post_inference_processing\n",
    "from speed_trapv3.tracking.tracking import get_video_properties, transform_image, write_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/code/data/datasets/common_hall/source_video/25_resampled_vid.mp4\"\n",
    "save_path = video_path = \"/code/data/datasets/common_hall/source_video/25_resampled_vid.mp4\"\n",
    "save_path = '/code/data/datasets/common_hall/tracking_inference_frame_lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf /code/data/datasets/common_hall/tracking_inference_frame_lib/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps, n_frames = get_video_properties(video_path)\n",
    "# reader = imageio.get_reader(video_path)\n",
    "# slug = \"hard_coded\"\n",
    "# for i in tqdm(range(n_frames)):\n",
    "#     img = reader.get_data(i)\n",
    "#     img = cv2.rectangle(\n",
    "#         img, (450, 200), (1280, 720), thickness=5, color=(0, 255, 0))\n",
    "#     filename = str(i) + \".jpg\"\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     cv2.imwrite(os.path.join(save_path, filename), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint_model = SegmentationModel().eval().cuda()\n",
    "keypoint_model.load(KeyConfig.trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert object to 'str' for 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m boxes \u001b[39m=\u001b[39m obj[\u001b[39m'\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m frame_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(frame_start, frame_end):\n\u001b[1;32m      8\u001b[0m         \u001b[39m# img = imageio.imread(Path(save_path)/f'{frame_idx}.jpg')\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(Path(save_path)\u001b[39m/\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mframe_idx\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m         box \u001b[39m=\u001b[39m boxes[\u001b[39mstr\u001b[39m(frame_idx)][\u001b[39m'\u001b[39m\u001b[39mbounding_box\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m         x \u001b[39m=\u001b[39m box[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert object to 'str' for 'filename'"
     ]
    }
   ],
   "source": [
    "f = open('/code/data/datasets/tracking/predictions/hard_coded_vehicle.json')\n",
    "data = json.load(f)\n",
    "for obj in data['annotations']:\n",
    "    obj_id = obj['id']\n",
    "    frame_start, frame_end = obj['segments'][0]\n",
    "    boxes = obj['frames']\n",
    "    for frame_idx in range(frame_start, frame_end):\n",
    "            # img = imageio.imread(Path(save_path)/f'{frame_idx}.jpg')\n",
    "            img = cv2.imread(str(Path(save_path)/f'{frame_idx}.jpg'))\n",
    "            box = boxes[str(frame_idx)]['bounding_box']\n",
    "            x = box['x']\n",
    "            y = box['y']\n",
    "            w = box['w']\n",
    "            h = box['h']\n",
    "            box = Boxes([x, y, w, h]).to_tlbr()\n",
    "            x1, y1, x2, y2 = box\n",
    "            roi_w = x2 - x1\n",
    "            roi_h = y2 - y1\n",
    "            roi_resized = crop_and_resize(box, img, KeyConfig.image_crop_size[0],KeyConfig.image_crop_size[1])\n",
    "            roi_resized_w, roi_resized_h = roi_resized.size\n",
    "            # x = image_transform(roi_resized)\n",
    "            # x = torch.unsqueeze(x, 0).cuda()\n",
    "            # keypoints = keypoint_model(x)['keypoints'][0].detach().cpu().numpy()\n",
    "            keypoints = keypoint_model(roi_resized)\n",
    "            keypoints = keypoints_post_inference_processing(\n",
    "                keypoints, roi_resized_w, roi_resized_h, roi_w, roi_h, x1, y1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyConfig.image_crop_size[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
